{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade kagglehub -q\n",
    "!pip install ipywidgets -q\n",
    "!pip install tensorflow-cpu -q\n",
    "!pip install tensorflow_datasets -q\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu -q\n",
    "!pip install git+https://github.com/felafax/gemma.git -q\n",
    "!pip install qax -q\n",
    "!pip install jax-lorax -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_CACHE'] = '/mnt/persistent-disk/hf/'\n",
    "os.environ['HF_HOME'] = '/mnt/persistent-disk/hf/'\n",
    "!export HF_HUB_CACHE=\"/mnt/persistent-disk/hf/\"\n",
    "!export HF_HOME=\"/mnt/persistent-disk/hf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yWaP_LPoEcoY"
   },
   "outputs": [],
   "source": [
    "# @title Python imports\n",
    "\n",
    "import enum\n",
    "import re\n",
    "import string\n",
    "import pdb\n",
    "\n",
    "# We import JAX and some related packages.\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from functools import partial\n",
    "\n",
    "# For LoRA\n",
    "import lorax\n",
    "\n",
    "# We will use HuggingFace's dataset, tokenizer, and model classes.\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, default_data_collator\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import torch\n",
    "\n",
    "# Finally, we import Gemma.\n",
    "from gemma import params as params_lib\n",
    "from gemma import sampler as sampler_lib\n",
    "from gemma import transformer as transformer_lib\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VsT2o6JEcoZ"
   },
   "source": [
    "## Fine tuning the Gemma model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax.traverse_util import flatten_dict\n",
    "from flax.core.meta import unbox\n",
    "\n",
    "\n",
    "def print_params(params):\n",
    "    flat_params = flatten_dict(params)    \n",
    "    for path, param in flat_params.items():\n",
    "        # Join the path components to create a string name\n",
    "        name = \"/\".join(str(x) for x in path)\n",
    "        print(f\"Name: {name}\")\n",
    "        # print(f\"Shape: {param.shape}\")\n",
    "        # print(f\"dtype: {param.dtype}\")\n",
    "        # print(f\"Value: {param}\")\n",
    "        if isinstance(param, flax.core.meta.Partitioned):\n",
    "            array = unbox(param)\n",
    "        else:\n",
    "            array = param\n",
    "        print(jax.debug.visualize_array_sharding(array))\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try LoRA with simpleNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 32 \n",
    "hidden_dim = 8\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function for creating NamedSharding\n",
    "def create_sharding(pspec):\n",
    "    return NamedSharding(mesh, pspec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mesh_sharding(pspec: PartitionSpec) -> NamedSharding:\n",
    "  return NamedSharding(mesh, pspec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "from jax.lax import with_sharding_constraint\n",
    "from jax.experimental import mesh_utils\n",
    "import functools\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = with_sharding_constraint(x, mesh_sharding(PartitionSpec('data', 'model')))\n",
    "\n",
    "        x = nn.Dense(features=self.hidden_dim, \n",
    "                     kernel_init=nn.with_partitioning(nn.initializers.xavier_normal(), ('data', 'model')),\n",
    "                     use_bias=False)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.output_dim, \n",
    "                     use_bias=False)(x)\n",
    "        return x\n",
    "\n",
    "# Set up the device mesh\n",
    "devices = jax.devices()\n",
    "device_mesh = mesh_utils.create_device_mesh((1, 4))\n",
    "mesh = Mesh(devices=device_mesh, axis_names=('data', 'model'))\n",
    "\n",
    "print(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = jnp.ones(shape=(1, input_dim))\n",
    "x_sharding = mesh_sharding(PartitionSpec('data', 'model')) # dimensions: (batch, length)\n",
    "sample_batch = jax.device_put(sample_batch, x_sharding)\n",
    "jax.debug.visualize_array_sharding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize function\n",
    "def init_fn(key, x, model, optimizer):\n",
    "    params = model.init(key, x)  # Initialize the model\n",
    "    state = train_state.TrainState.create(  # Create a `TrainState`\n",
    "        apply_fn=model.apply,\n",
    "        params=params['params'],\n",
    "        tx=optimizer)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(hidden_dim, output_dim)\n",
    "optimizer = optax.adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_variables = jax.eval_shape(\n",
    "    functools.partial(\n",
    "        init_fn, \n",
    "        model=model, \n",
    "        optimizer=optimizer\n",
    "    ),\n",
    "    jax.random.PRNGKey(99),\n",
    "    sample_batch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sharding = nn.get_sharding(abstract_variables, mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_init_fn = jax.jit(init_fn, \n",
    "                          static_argnums=(2, 3),\n",
    "                          in_shardings=(mesh_sharding(pspec=()), x_sharding),  # PRNG key and x\n",
    "                          out_shardings=state_sharding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialized_state = jit_init_fn(jax.random.PRNGKey(99), sample_batch, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialized_state.params['Dense_1']['kernel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_params(initialized_state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(params, state, batch):\n",
    "  input_images, labels = batch\n",
    "    \n",
    "  # call forward pass function.\n",
    "  logits = state.apply_fn({\"params\": params}, input_images)\n",
    "\n",
    "  # compute loss\n",
    "  loss = optax.squared_error(logits, labels)\n",
    "  loss = loss.mean()\n",
    "  return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(state, batch):\n",
    "  # create a function to compute gradients wrt to loss\n",
    "  # returned by our `forward_pass` function.\n",
    "  grad_fn = jax.value_and_grad(forward_pass, argnums=(0), has_aux=True)\n",
    "\n",
    "  # compute gradients.\n",
    "  (loss, _), grads = grad_fn(state.params, state, batch)\n",
    "\n",
    "  # apply gradients.\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding, None),\n",
    "                   out_shardings=state_sharding)\n",
    "def train_step(state, inputs, targets):\n",
    "  return backward_pass(state, (inputs, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = (jnp.ones(shape=(1, input_dim)), jnp.zeros(shape=(1, output_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch0 = jax.device_put(batch[0], x_sharding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = jax.device_put(batch[1], mesh_sharding(PartitionSpec('data', None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mesh:\n",
    "    new_state = train_step(initialized_state, batch0, batch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_params(params):\n",
    "    flat_params = flatten_dict(params)    \n",
    "    for path, param in flat_params.items():\n",
    "        # Join the path components to create a string name\n",
    "        name = \"/\".join(str(x) for x in path)\n",
    "        print(f\"Name: {name}\")\n",
    "        print(f\"Shape: {param.shape}\")\n",
    "        print(f\"dtype: {param.dtype}\")\n",
    "        print(f\"Value: {param}\")\n",
    "        # if isinstance(param, flax.core.meta.Partitioned):\n",
    "        #     array = unbox(param)\n",
    "        # else:\n",
    "        #     array = param\n",
    "        # print(jax.debug.visualize_array_sharding(array))\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "from jax.lax import with_sharding_constraint\n",
    "from jax.experimental import mesh_utils\n",
    "import functools\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental.pjit import pjit\n",
    "from jax.sharding import Mesh, PartitionSpec as PS\n",
    "from flax import linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import optax\n",
    "from functools import partial\n",
    "from jax.experimental import mesh_utils\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = with_sharding_constraint(x, mesh_sharding(PartitionSpec('data', 'model')))\n",
    "\n",
    "        x = nn.Dense(features=self.hidden_dim, \n",
    "                     kernel_init=nn.with_partitioning(nn.initializers.xavier_normal(), ('data', 'model')),\n",
    "                     use_bias=False)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.output_dim, \n",
    "                     use_bias=False)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the device mesh\n",
    "devices = jax.devices()\n",
    "device_mesh = mesh_utils.create_device_mesh((1, 4))\n",
    "mesh = Mesh(devices=device_mesh, axis_names=('data', 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleNN(\u001b[43mhidden_dim\u001b[49m, output_dim)\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39madam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hidden_dim' is not defined"
     ]
    }
   ],
   "source": [
    "model = SimpleNN(hidden_dim, output_dim)\n",
    "optimizer = optax.adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class SimpleNN(nn.Module):\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=self.hidden_dim, use_bias=False)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.output_dim, use_bias=False)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaxRNG(object):\n",
    "    \"\"\" A convenient stateful Jax RNG wrapper. Can be used to wrap RNG inside\n",
    "        pure function.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_seed(cls, seed):\n",
    "        return cls(jax.random.PRNGKey(seed))\n",
    "\n",
    "    def __init__(self, rng):\n",
    "        self.rng = rng\n",
    "\n",
    "    def __call__(self, keys=None):\n",
    "        if keys is None:\n",
    "            self.rng, split_rng = jax.random.split(self.rng)\n",
    "            return split_rng\n",
    "        elif isinstance(keys, int):\n",
    "            split_rngs = jax.random.split(self.rng, num=keys + 1)\n",
    "            self.rng = split_rngs[0]\n",
    "            return tuple(split_rngs[1:])\n",
    "        else:\n",
    "            split_rngs = jax.random.split(self.rng, num=len(keys) + 1)\n",
    "            self.rng = split_rngs[0]\n",
    "            return {key: val for key, val in zip(keys, split_rngs[1:])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_path_to_string(path, sep=None):\n",
    "    keys = []\n",
    "    for key in path:\n",
    "        if isinstance(key, jax.tree_util.SequenceKey):\n",
    "            keys.append(str(key.idx))\n",
    "        elif isinstance(key, jax.tree_util.DictKey):\n",
    "            keys.append(str(key.key))\n",
    "        elif isinstance(key, jax.tree_util.GetAttrKey):\n",
    "            keys.append(str(key.name))\n",
    "        elif isinstance(key, jax.tree_util.FlattenedIndexKey):\n",
    "            keys.append(str(key.key))\n",
    "        else:\n",
    "            keys.append(str(key))\n",
    "    if sep is None:\n",
    "        return tuple(keys)\n",
    "    return sep.join(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_tree_map(f, tree, *rest, is_leaf=None, sep=None):\n",
    "    \"\"\" An extended version of jax.tree_util.tree_map, where the mapped function\n",
    "        f takes both the name (path) and the tree leaf as input.\n",
    "    \"\"\"\n",
    "    return jax.tree_util.tree_map_with_path(\n",
    "        lambda path, x, *r: f(tree_path_to_string(path, sep=sep), x, *r),\n",
    "        tree, *rest,\n",
    "        is_leaf=is_leaf\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def match_partition_rules(rules, params):\n",
    "    \"\"\" Returns a pytree of PartitionSpec according to rules. Supports handling\n",
    "        Flax TrainState and Optax optimizer state.\n",
    "    \"\"\"\n",
    "    def get_partition_spec(name, leaf):\n",
    "        if len(leaf.shape) == 0 or np.prod(leaf.shape) == 1:\n",
    "            \"\"\" Don't partition scalar values. \"\"\"\n",
    "            return PS()\n",
    "        for rule, ps in rules:\n",
    "            if re.search(rule, name) is not None:\n",
    "                return ps\n",
    "        raise ValueError(f'Partition rule not found for param: {name}')\n",
    "    return named_tree_map(get_partition_spec, params, sep='/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jax_mesh(axis_dims, names):\n",
    "    dims = [int(x) for x in axis_dims.split(',')]\n",
    "    mesh_shape = jax.numpy.arange(jax.device_count()).reshape(dims).shape\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_sharding_constraint(x, partition_specs):\n",
    "    return jax.lax.with_sharding_constraint(x, partition_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "input_dim = 32 \n",
    "hidden_dim = 8\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNNConfigurator:\n",
    "    @staticmethod\n",
    "    def get_partition_rules():\n",
    "        return (\n",
    "            # (\"params/params/Dense_0/kernel\", PS(\"data\", \"model\")),\n",
    "            ('.*', PS(None)),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def rng_keys():\n",
    "        return ('params', 'dropout', 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainstate_from_params(params):\n",
    "    return TrainState.create(params=params, tx=optimizer, apply_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_fn(rng, input_dim=32, hidden_dim=8, output_dim=1):\n",
    "    rng_generator = JaxRNG(rng)\n",
    "    model = SimpleNN(hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    params = model.init(\n",
    "        rng_generator(SimpleNNConfigurator.rng_keys()),\n",
    "        jnp.zeros((4, input_dim)),\n",
    "    )\n",
    "    return TrainState.create(params=params, tx=optimizer, apply_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_state, rng, batch):\n",
    "    rng_generator = JaxRNG(rng)\n",
    "    batch = jax.lax.with_sharding_constraint(batch, PS(('data', 'model')))\n",
    "    \n",
    "    def loss_and_accuracy(params):\n",
    "        pred = model.apply(params, batch['input'],\n",
    "            rngs=rng_generator(SimpleNNConfigurator.rng_keys()),\n",
    "        )\n",
    "        loss = optax.squared_error(pred, batch['target'])\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "        \n",
    "    grad_fn = jax.grad(loss_and_accuracy)\n",
    "    grads = grad_fn(train_state.params)\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = JaxRNG(0)\n",
    "model = SimpleNN(hidden_dim=hidden_dim, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_shape(fun: Callable, *args, **kwargs):\n",
    "# evaluating ``fun(*args, **kwargs)``.\n",
    "\n",
    "train_state_shapes = jax.eval_shape(init_fn, jax.random.PRNGKey(99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state_partition = match_partition_rules(\n",
    "    SimpleNNConfigurator.get_partition_rules(), train_state_shapes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_named_sharding(partition_spec):\n",
    "    return NamedSharding(mesh, partition_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state_named_sharding = jax.tree.map(create_named_sharding, train_state_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'Dense_0': {'kernel': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))},\n",
       "  'Dense_1': {'kernel': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state_named_sharding.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharded_init_fn = jax.jit(\n",
    "    init_fn,\n",
    "    in_shardings=NamedSharding(mesh, PS()),\n",
    "    # out_shardings=train_state_partition, out_shardings is optional in jax.jit, GSPMD will figure it out.\n",
    "    static_argnums=(1, 2, 3)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharded_create_trainstate_from_params = jax.jit(\n",
    "    create_trainstate_from_params,\n",
    "    in_shardings=(train_state_partition.params, ),\n",
    "    # out_shardings=train_state_partition,\n",
    "    donate_argnums=(0, ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharded_train_step = jax.jit(\n",
    "    train_step,\n",
    "    in_shardings=(train_state_named_sharding, NamedSharding(mesh, PS()), NamedSharding(mesh, PS())),\n",
    "    # out_shardings=(train_state_partition),\n",
    "    donate_argnums=(0, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_mesh = mesh_utils.create_device_mesh((1, 4))\n",
    "mesh = Mesh(devices=device_mesh, axis_names=('data', 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainState(step=Array(1, dtype=int32, weak_type=True), apply_fn=None, params={'params': {'Dense_0': {'kernel': Array([[ 9.58651379e-02, -5.77238090e-02,  8.71751755e-02,\n",
      "         2.06664905e-01, -1.54080182e-01,  3.74663025e-01,\n",
      "         1.21506892e-01, -1.27760872e-01],\n",
      "       [-1.65383801e-01,  9.92371961e-02, -6.61521703e-02,\n",
      "         2.75267810e-01,  3.34123582e-01,  1.94328249e-01,\n",
      "        -2.11523082e-02,  7.46464133e-02],\n",
      "       [ 6.08712770e-02, -1.41109407e-01, -8.76355544e-02,\n",
      "         2.75042266e-01, -2.81154457e-02, -1.16207547e-01,\n",
      "        -2.79863566e-01,  2.71764964e-01],\n",
      "       [ 1.61056504e-01, -9.62803438e-02,  3.30342233e-01,\n",
      "        -3.67344916e-01,  4.07360569e-02, -1.63870811e-01,\n",
      "        -1.81345776e-01,  4.84794006e-03],\n",
      "       [-9.79540274e-02,  1.76957369e-01,  3.66404593e-01,\n",
      "        -1.52017111e-02,  5.93522601e-02,  9.57616698e-03,\n",
      "         9.57029611e-02, -1.66373268e-01],\n",
      "       [ 7.82506913e-02,  2.19656542e-01,  4.44784090e-02,\n",
      "         3.93143147e-02,  1.70469340e-02,  1.64118275e-01,\n",
      "         1.50957823e-01, -2.46063903e-01],\n",
      "       [ 6.97808936e-02, -2.00764596e-01, -2.46128127e-01,\n",
      "         2.44701609e-01, -1.63323041e-02,  8.06982070e-03,\n",
      "        -2.96295464e-01,  1.10650219e-01],\n",
      "       [ 1.70194536e-01, -2.68165339e-02, -1.08412966e-01,\n",
      "        -5.80477640e-02, -3.49546336e-02,  4.79427613e-02,\n",
      "        -1.16684221e-01,  7.38132671e-02],\n",
      "       [-4.86243144e-02, -1.33826360e-01,  6.28300086e-02,\n",
      "         4.83388342e-02,  9.65549275e-02,  1.33059949e-01,\n",
      "         8.96632764e-03,  1.22078434e-01],\n",
      "       [-2.09511127e-02,  1.14838257e-01,  1.73385859e-01,\n",
      "         4.35407571e-02,  6.99398294e-02,  4.18988615e-02,\n",
      "         5.56956753e-02,  2.09704898e-02],\n",
      "       [ 6.89942855e-03,  1.03794876e-02, -1.20036125e-01,\n",
      "        -5.71607538e-02,  3.24923933e-01, -2.40171030e-02,\n",
      "         1.20772272e-02,  2.81197757e-01],\n",
      "       [-5.77472746e-02,  9.17002857e-02, -8.06393474e-02,\n",
      "        -2.31510893e-01,  7.36167058e-02,  2.92208940e-01,\n",
      "        -2.48447686e-01,  2.94839412e-01],\n",
      "       [-2.46368811e-01, -1.23847269e-01,  1.14577357e-02,\n",
      "        -1.43822134e-01,  2.46667802e-01, -1.40820205e-01,\n",
      "        -1.27565535e-02,  3.62484932e-01],\n",
      "       [-3.97865325e-02, -3.91432345e-02, -2.32120633e-01,\n",
      "        -2.80889124e-01,  1.74350828e-01, -3.55541334e-02,\n",
      "        -1.30411461e-01, -3.93446647e-02],\n",
      "       [-8.70884433e-02, -1.71413273e-03,  1.10669822e-01,\n",
      "        -1.50006041e-01,  9.38230529e-02, -3.07864338e-01,\n",
      "         2.42998004e-02,  5.59182875e-02],\n",
      "       [ 2.65604615e-01,  1.33880109e-01,  6.30205823e-03,\n",
      "        -2.38418728e-01,  2.27939278e-01, -2.98713803e-01,\n",
      "         2.56680310e-01,  3.87004346e-01],\n",
      "       [ 3.10103238e-01,  4.48181741e-02,  1.55153796e-01,\n",
      "         9.16200206e-02,  2.46349856e-01, -2.24562716e-02,\n",
      "        -1.14756174e-01, -4.45334688e-02],\n",
      "       [-1.79921836e-01, -9.40382257e-02, -1.13400564e-01,\n",
      "        -5.20668216e-02, -7.54378363e-02,  2.28565812e-01,\n",
      "         5.85835390e-02, -1.31987274e-01],\n",
      "       [-2.51752557e-04,  1.65260211e-01, -3.18870455e-01,\n",
      "        -1.91721722e-01,  1.18839145e-01, -2.09571630e-01,\n",
      "         5.47039583e-02,  1.93867907e-01],\n",
      "       [ 3.12005460e-01, -1.32446453e-01, -1.62940502e-01,\n",
      "         1.56320974e-01, -2.46608019e-01, -2.96641830e-02,\n",
      "        -3.43344151e-03,  2.97642291e-01],\n",
      "       [-1.67611614e-01, -7.98916630e-03, -3.96195846e-03,\n",
      "        -3.23131114e-01, -1.30249396e-01, -2.54046649e-01,\n",
      "        -7.49458894e-02, -1.31732047e-01],\n",
      "       [ 1.61187902e-01, -1.75606951e-01,  1.65372223e-01,\n",
      "         1.31592259e-01,  2.41059288e-01, -5.31785935e-02,\n",
      "        -9.44162831e-02,  2.10415259e-01],\n",
      "       [ 3.30500543e-01, -2.42979169e-01, -2.07141772e-01,\n",
      "        -2.35203832e-01, -2.09055617e-01, -7.46918470e-02,\n",
      "        -2.75038974e-03, -8.07918012e-02],\n",
      "       [-1.48183778e-01,  3.31676781e-01,  3.84180427e-01,\n",
      "         2.00133651e-01, -3.59694690e-01,  1.35231599e-01,\n",
      "         7.27689490e-02,  2.10119769e-01],\n",
      "       [ 1.34801697e-02, -1.36472210e-01, -1.11252077e-01,\n",
      "        -2.17809439e-01, -7.47935101e-02,  6.40343577e-02,\n",
      "         3.52699533e-02, -5.63642047e-02],\n",
      "       [-2.13968411e-01,  3.98997813e-01,  3.86152640e-02,\n",
      "         1.36926323e-01,  3.91404063e-01,  1.16414286e-01,\n",
      "        -9.56563726e-02,  1.15589812e-01],\n",
      "       [ 1.78841531e-01,  2.58336008e-01, -3.64756942e-01,\n",
      "         3.86025757e-02,  2.00053871e-01, -3.94380875e-02,\n",
      "         1.38307437e-01, -5.07196784e-02],\n",
      "       [-2.80744046e-01,  1.55669257e-01,  1.36660233e-01,\n",
      "         2.53176242e-01,  2.70596951e-01,  6.63685575e-02,\n",
      "         8.58785734e-02,  3.19406986e-01],\n",
      "       [ 9.09672454e-02,  2.30683252e-01, -3.23533028e-01,\n",
      "         2.83477128e-01,  4.71560173e-02, -4.11243457e-03,\n",
      "        -1.45658376e-02,  1.37909018e-02],\n",
      "       [ 1.09404594e-01, -2.29936000e-02, -9.60382521e-02,\n",
      "        -1.63592317e-03,  8.21953565e-02, -2.28100926e-01,\n",
      "        -1.44134134e-01,  2.87574083e-01],\n",
      "       [-2.13150844e-01, -2.15589717e-01,  2.70242803e-02,\n",
      "        -4.37777303e-02,  2.43651435e-01,  1.66691601e-01,\n",
      "        -1.03152141e-01,  3.25182348e-01],\n",
      "       [ 1.12266302e-01,  1.03202395e-01,  1.84591889e-01,\n",
      "        -7.64375255e-02, -1.08447969e-01,  1.70533508e-02,\n",
      "         1.92491814e-01, -2.37895682e-01]], dtype=float32)}, 'Dense_1': {'kernel': Array([[-0.12892883],\n",
      "       [ 0.48477697],\n",
      "       [ 0.30282158],\n",
      "       [ 0.36617175],\n",
      "       [ 0.15251277],\n",
      "       [-0.42674887],\n",
      "       [-0.05488507],\n",
      "       [ 0.11635382]], dtype=float32)}}}, tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x7f1b0c59f6d0>, update=<function chain.<locals>.update_fn at 0x7f1a8c209ab0>), opt_state=(ScaleByAdamState(count=Array(1, dtype=int32), mu={'params': {'Dense_0': {'kernel': Array([[-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734]], dtype=float32)}, 'Dense_1': {'kernel': Array([[0.09795997],\n",
      "       [0.1329792 ],\n",
      "       [0.        ],\n",
      "       [0.        ],\n",
      "       [0.40819302],\n",
      "       [0.00450534],\n",
      "       [0.        ],\n",
      "       [0.51165694]], dtype=float32)}}}, nu={'params': {'Dense_0': {'kernel': Array([[5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05]],      dtype=float32)}, 'Dense_1': {'kernel': Array([[9.5961551e-04],\n",
      "       [1.7683469e-03],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [1.6662154e-02],\n",
      "       [2.0298103e-06],\n",
      "       [0.0000000e+00],\n",
      "       [2.6179284e-02]], dtype=float32)}}}), EmptyState()))\n"
     ]
    }
   ],
   "source": [
    "with mesh:\n",
    "    train_state = sharded_init_fn(jax.random.PRNGKey(99))\n",
    "    batch = {\n",
    "        'input': jnp.ones((32, input_dim)),\n",
    "        'target': jnp.zeros((32, 1), dtype=jnp.int32),\n",
    "    }\n",
    "    sharded_rng = jax.random.PRNGKey(99)\n",
    "    train_state = sharded_train_step(\n",
    "        train_state, sharded_rng, batch\n",
    "    )\n",
    "    print(train_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
