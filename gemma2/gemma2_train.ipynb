{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5OeTiryEcoX"
   },
   "source": [
    "# Fine-tuning Gemma2 2B model on Roadrunner with JAX, Flax.\n",
    "\n",
    "We have adopted the Gemma2 notebook from Google Deepmind to use HuggingFace's libraries and and simplified the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m81VQOqEcoX"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade kagglehub -q\n",
    "!pip install ipywidgets -q\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cpu -q\n",
    "!pip install git+https://github.com/felafax/gemma.git -q\n",
    "!pip install qax -q\n",
    "!pip install jax-lorax -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_CACHE'] = '/mnt/persistent-disk/hf/'\n",
    "os.environ['HF_HOME'] = '/mnt/persistent-disk/hf/'\n",
    "!export HF_HUB_CACHE=\"/mnt/persistent-disk/hf/\"\n",
    "!export HF_HOME=\"/mnt/persistent-disk/hf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "yWaP_LPoEcoY"
   },
   "outputs": [],
   "source": [
    "# @title Python imports\n",
    "\n",
    "import enum\n",
    "import re\n",
    "import string\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# We import JAX and some related packages.\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.traverse_util import flatten_dict\n",
    "from flax.core.meta import unbox\n",
    "\n",
    "import optax\n",
    "import functools\n",
    "from functools import partial\n",
    "\n",
    "# Model partitioning related imports\n",
    "from jax.sharding import Mesh, NamedSharding\n",
    "from jax.sharding import PartitionSpec as P\n",
    "from jax.lax import with_sharding_constraint\n",
    "from jax.experimental import mesh_utils\n",
    "\n",
    "\n",
    "# For LoRA\n",
    "import lorax\n",
    "\n",
    "# We will use HuggingFace's dataset, tokenizer, and model classes.\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, default_data_collator\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import torch\n",
    "\n",
    "# Finally, we import Gemma.\n",
    "from gemma import params as params_lib\n",
    "from gemma import sampler as sampler_lib\n",
    "from gemma import transformer as transformer_lib\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "INPUT: Please provide your HUGGINGFACE_USERNAME:  felarof01\n",
      "INPUT: Please provide your HUGGINGFACE_TOKEN:  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\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace username and token to use when downloading.\n",
    "MODEL_NAME=\"felafax/gemma-2-2b-it-Flax\"\n",
    "HUGGINGFACE_USERNAME = input(\"INPUT: Please provide your HUGGINGFACE_USERNAME: \")\n",
    "HUGGINGFACE_TOKEN = input(\"INPUT: Please provide your HUGGINGFACE_TOKEN: \")\n",
    "\n",
    "model_name=MODEL_NAME\n",
    "hugging_face_token=HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "ckpt_path = snapshot_download(repo_id=MODEL_NAME, token=HUGGINGFACE_TOKEN)\n",
    "vocab_path = os.path.join(ckpt_path, 'tokenizer.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VsT2o6JEcoZ"
   },
   "source": [
    "## Fine tuning the Gemma model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax.traverse_util import flatten_dict\n",
    "\n",
    "def print_params(params):\n",
    "    flat_params = flatten_dict(params)    \n",
    "    for key, value in flat_params.items():\n",
    "        name = \"/\".join(str(x) for x in key)\n",
    "        print(f\"Name: {name}\")\n",
    "        # print(f\"Shape: {value.shape}\")\n",
    "        # print(f\"dtype: {value.dtype}\")\n",
    "        # print(f\"Value: {value}\")\n",
    "        if isinstance(value, flax.core.meta.Partitioned):\n",
    "            array = unbox(value)\n",
    "        else:\n",
    "            array = value\n",
    "        print(jax.debug.visualize_array_sharding(array))\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: prepare the dataset\n",
    "\n",
    "For this project, we're utilizing the refined **Alpaca dataset**, curated by yahma. This dataset is a carefully filtered selection of 52,000 entries from the original Alpaca collection. Feel free to substitute this section with your own data preparation code if you prefer.\n",
    "\n",
    "It's crucial to include the EOS_TOKEN (End of Sequence Token) in your tokenized output. Failing to do so may result in endless generation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(*, tokenizer, batch_size=1, max_length=32, max_examples=32):\n",
    "    # Define Alpaca prompt template\n",
    "    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "    \n",
    "    ### Instruction: {}\n",
    "    \n",
    "    ### Input: {}\n",
    "    \n",
    "    ### Response: {}\"\"\"\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    \n",
    "    # Define formatting function.\n",
    "    def _format_prompts(examples):\n",
    "        instructions = examples[\"instruction\"]\n",
    "        inputs = examples[\"input\"]\n",
    "        outputs = examples[\"output\"]\n",
    "        texts = []\n",
    "        for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "            text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    def _tokenize(examples):\n",
    "        tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length+1)\n",
    "        tokenized['input_ids'] = [input_id[:-1] for input_id in tokenized['input_ids']]\n",
    "        tokenized['target_mask'] = [input_id[:-1] for input_id in tokenized['attention_mask']]\n",
    "        return {\n",
    "            'input_tokens': tokenized['input_ids'],\n",
    "            'target_mask': tokenized['target_mask']\n",
    "        }\n",
    "\n",
    "    def _custom_collate_fn(batch):\n",
    "        \"\"\"Applies default_collate_fn from transformers and converts to JAX NumPy arrays.\"\"\"\n",
    "        batch = default_data_collator(batch)\n",
    "        jax_batch = {}\n",
    "        for key, value in batch.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                jax_batch[key] = jnp.array(value.numpy())\n",
    "            else:\n",
    "                jax_batch[key] = value\n",
    "        \n",
    "        return jax_batch\n",
    "\n",
    "    # Load and preprocess the dataset.\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "    if max_examples:\n",
    "        dataset = dataset.select(range(max_examples))\n",
    "    dataset = dataset.map(_format_prompts, batched=True)\n",
    "\n",
    "    # Create train and test dataset.\n",
    "    ds = dataset.train_test_split(test_size=0.15)\n",
    "    ds['train'] = ds['train'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "    ds['test'] = ds['test'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['train'],\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=_custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['test'],\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=_custom_collate_fn\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6a0a48b85f4c97b3a144c9af0438a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72d95d0a49442bcb5461101ce88d1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32)\n",
      "\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "\n",
      "(1, 32)\n"
     ]
    }
   ],
   "source": [
    "# # # Test Dataset\n",
    "train_dataloader, _ = get_dataset(tokenizer=tokenizer)\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    if i>10:\n",
    "        break\n",
    "    input_ids, attention_mask = (\n",
    "        batch[\"input_tokens\"],\n",
    "        batch[\"target_mask\"],\n",
    "        \n",
    "    )\n",
    "    print(input_ids.shape)\n",
    "    print()\n",
    "    print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "cellView": "form",
    "id": "iEcV0XEEEcoZ"
   },
   "outputs": [],
   "source": [
    "def forward_and_loss_fn(params,\n",
    "                        *,\n",
    "                        state,\n",
    "                        input_tokens: jax.Array,            # Shape [B, L]\n",
    "                        input_mask: jax.Array,              # Shape [B, L]\n",
    "                        positions: jax.Array,               # Shape [B, L]\n",
    "                        attention_mask: jax.Array,          # [B, L, L]\n",
    "                        ) -> jax.Array:\n",
    "  \"\"\"Forward pass and loss function.\n",
    "\n",
    "  Args:\n",
    "    params: model's input parameters.\n",
    "    model: gemma transformer model to call.\n",
    "    input_tokens: input tokens sequence, shape [B, L].\n",
    "    input_mask: tokens to ignore when computing the loss, shape [B, L].\n",
    "    positions: relative position of each token, shape [B, L].\n",
    "    attention_mask: input attention mask, shape [B, L].\n",
    "\n",
    "  Returns:\n",
    "    Softmax cross-entropy loss for the next-token prediction task.\n",
    "  \"\"\"\n",
    "\n",
    "  # Forward pass on the input data.\n",
    "  # No attention cache is needed here.\n",
    "  logits, _ = state.apply_fn(\n",
    "        params,\n",
    "        input_tokens,\n",
    "        positions,\n",
    "        None,              # Attention cache is None.\n",
    "        attention_mask,\n",
    "    )\n",
    "\n",
    "  # Exclude the last step as it does not appear in the targets.\n",
    "  logits = logits[:, :-1]\n",
    "\n",
    "  # Similarly, the first token cannot be predicteds.\n",
    "  target_tokens = input_tokens[:, 1:]\n",
    "  target_mask = input_mask[:, 1:]\n",
    "\n",
    "  # Convert the target labels into one-hot encoded vectors.\n",
    "  one_hot = jax.nn.one_hot(target_tokens, logits.shape[-1])\n",
    "\n",
    "  # Don't update on unwanted tokens.\n",
    "  one_hot = one_hot * target_mask.astype(one_hot.dtype)[..., None]\n",
    "\n",
    "  # Normalisation factor.\n",
    "  norm_factor = 1 / (jnp.sum(target_mask) + 1e-8)\n",
    "\n",
    "  # Return the nll loss.\n",
    "  return -jnp.sum(jax.nn.log_softmax(logits) * one_hot) * norm_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y83DimpjEcoZ"
   },
   "source": [
    "The Gemma transformer requires an attention mask and position vector alongside each input. We can conveniently generate these using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "cellView": "form",
    "id": "cbWfdHf0EcoZ"
   },
   "outputs": [],
   "source": [
    "def get_attention_mask_and_positions(example: jax.Array,\n",
    "                                     pad_id : int,\n",
    "                                     )-> tuple[jax.Array, jax.Array]:\n",
    "  \"\"\"Builds the position and attention mask vectors from the given tokens.\"\"\"\n",
    "  pad_mask = example != pad_id\n",
    "  current_token_position = transformer_lib.build_positions_from_mask(pad_mask)\n",
    "  attention_mask = transformer_lib.make_causal_attn_mask(pad_mask)\n",
    "  return current_token_position, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbxYMMWLEcoZ"
   },
   "source": [
    "We can now build the train_step function which performs the backward pass and updates the model's parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "cellView": "form",
    "id": "cPSfp7ZUEcoZ"
   },
   "outputs": [],
   "source": [
    "def train_step(state,\n",
    "               params,\n",
    "               optimizer: optax.GradientTransformation,\n",
    "               opt_state: optax.OptState,\n",
    "               pad_id: int,\n",
    "               batch):\n",
    "  \"\"\"Train step.\n",
    "\n",
    "  Args:\n",
    "    model: gemma transformer model.\n",
    "    params: model's input parameters.\n",
    "    optimizer: optax optimizer to use.\n",
    "    opt_state: input optimizer's state.\n",
    "    pad_id: id of the pad token.\n",
    "    batch: input batch.\n",
    "\n",
    "  Returns:\n",
    "    Training loss, updated parameters, updated optimizer state.\n",
    "  \"\"\"\n",
    "  # Build the position and attention mask vectors.\n",
    "  positions, attention_mask = get_attention_mask_and_positions(batch['input_tokens'], pad_id)\n",
    "\n",
    "  # Forward and backward passes\n",
    "  train_loss, grads = jax.value_and_grad(forward_and_loss_fn)(params,\n",
    "                                                             state=state,\n",
    "                                                             input_tokens=batch['input_tokens'],\n",
    "                                                             input_mask=batch['target_mask'],\n",
    "                                                             positions=positions,\n",
    "                                                             attention_mask=attention_mask)\n",
    "  # Update the parameters\n",
    "  # updates, opt_state = optimizer.update(grads, opt_state)\n",
    "  # params = optax.apply_updates(params, updates)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "\n",
    "  return train_loss, params, opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2QXp116EcoZ"
   },
   "source": [
    "Similarly, we build a `validation_step` function without backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6g6LFWJbEcoa"
   },
   "source": [
    "And now the training loop itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the device mesh\n",
    "devices = jax.devices()\n",
    "device_mesh = mesh_utils.create_device_mesh((1, 4))\n",
    "mesh = Mesh(devices=device_mesh, axis_names=('data', 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters.\n",
    "params = {\"params\": params_lib.load_and_format_params(os.path.join(ckpt_path, 'gemma2-2b-it'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model config.\n",
    "config = transformer_lib.TransformerConfig.gemma2_2b(cache_size=30)\n",
    "model = transformer_lib.Transformer(config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    token=HUGGINGFACE_TOKEN\n",
    ")\n",
    "optimizer = optax.sgd(training_cfg.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_fn(params, model, optimizer):\n",
    "    state = train_state.TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params['params'],\n",
    "        tx=optimizer)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_variables = jax.eval_shape(\n",
    "    functools.partial(\n",
    "        init_fn,\n",
    "        params=params,\n",
    "        model=model, \n",
    "        optimizer=optimizer\n",
    "    ),\n",
    ")\n",
    "abstract_sharded_state = nn.get_sharding(abstract_variables, mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_params_pytree(params, mesh):\n",
    "    def shard_param(param):\n",
    "        # Define sharding based on param shape\n",
    "        if len(param.shape) == 1:\n",
    "            # For 1D tensors (e.g., biases), shard across 'model' dimension\n",
    "            return NamedSharding(mesh, P('model'))\n",
    "        elif len(param.shape) == 2:\n",
    "            # For 2D tensors (e.g., weight matrices), shard across both dimensions\n",
    "            return NamedSharding(mesh, P('data', 'model'))\n",
    "        else:\n",
    "            # For higher-dimensional tensors, you might need a more complex strategy\n",
    "            return NamedSharding(mesh, P(None))  # Replicate by default\n",
    "\n",
    "    return jax.tree_util.tree_map(shard_param, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = shard_params_pytree(params, mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fn_jitted = jax.jit(init_fn, \n",
    "                         static_argnums=(0, 1, 2),\n",
    "                         out_shardings=abstract_sharded_state)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Non-hashable static arguments are not supported. An error occurred while trying to hash an object of type <class 'dict'>, {'params': {'transformer': {'embedder': {'input_embedding': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'final_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'layer_0': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_1': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_10': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_11': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_12': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_13': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_14': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_15': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_16': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_17': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_18': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_19': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_2': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_20': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_21': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_22': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_23': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_24': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_25': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_3': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_4': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_5': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_6': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_7': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_8': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_9': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}}}}. The error was:\nTypeError: unhashable type: 'dict'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mesh:\n\u001b[0;32m----> 2\u001b[0m     sharded_state \u001b[38;5;241m=\u001b[39m \u001b[43minit_fn_jitted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Non-hashable static arguments are not supported. An error occurred while trying to hash an object of type <class 'dict'>, {'params': {'transformer': {'embedder': {'input_embedding': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'final_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'layer_0': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_1': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_10': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_11': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_12': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_13': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_14': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_15': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_16': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_17': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_18': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_19': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_2': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_20': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_21': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_22': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_23': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_24': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_25': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_3': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_4': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_5': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_6': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_7': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_8': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}, 'layer_9': {'attn': {'attn_vec_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'kv_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}, 'q_einsum': {'w': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}, 'mlp': {'gating_einsum': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,)), 'linear': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('data', 'model'))}, 'post_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'post_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_attention_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}, 'pre_ffw_norm': {'scale': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec('model',))}}}}}. The error was:\nTypeError: unhashable type: 'dict'\n"
     ]
    }
   ],
   "source": [
    "with mesh:\n",
    "    sharded_state = init_fn_jitted(params, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = jax.device_put(sample_batch, NamedSharding(mesh, PartitionSpec('data', 'model')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_train_step = jax.jit(train_step, static_argnames=['model', 'optimizer'])\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, params, opt_state = train_step(model=model,\n",
    "                                            params=params,\n",
    "                                            optimizer=optimizer,\n",
    "                                            opt_state=opt_state,\n",
    "                                            pad_id=tokenizer.pad_token_id,\n",
    "                                            example=train_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cfg = TrainingConfig(learning_rate=1e-4,\n",
    "                              num_epochs=1,\n",
    "                              eval_every_n=20,\n",
    "                              batch_size=1,\n",
    "                              max_steps=10)\n",
    "\n",
    "params = train_loop(model=model_2b,\n",
    "                    params={'params': params['transformer']},\n",
    "                    train_dataloader=train_dataloader,\n",
    "                    tokenizer=tokenizer,\n",
    "                    training_cfg=training_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellView": "form",
    "id": "xT4bAqNLEcoa"
   },
   "outputs": [],
   "source": [
    "@chex.dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "  learning_rate: float\n",
    "  num_epochs: int\n",
    "  eval_every_n: int\n",
    "  batch_size: int\n",
    "  max_steps: int | None = None\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model: transformer_lib.Transformer,\n",
    "    params,\n",
    "    train_dataloader,\n",
    "    tokenizer,\n",
    "    training_cfg: TrainingConfig):\n",
    "\n",
    "\n",
    "  compiled_train_step = jax.jit(train_step, static_argnames=['model', 'optimizer'])\n",
    "  optimizer = optax.sgd(training_cfg.learning_rate)\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  n_steps = 0\n",
    "  avg_loss=0\n",
    "\n",
    "  for i, train_example in enumerate(train_dataloader):\n",
    "    train_loss, params, opt_state = train_step(model=model,\n",
    "                                                        params=params,\n",
    "                                                        optimizer=optimizer,\n",
    "                                                        opt_state=opt_state,\n",
    "                                                        pad_id=tokenizer.pad_token_id,\n",
    "                                                        example=train_example)\n",
    "    n_steps += 1\n",
    "    avg_loss += train_loss\n",
    "    print(f\"train_loss {train_loss}\")\n",
    "    if training_cfg.max_steps is not None and n_steps > training_cfg.max_steps:\n",
    "      break\n",
    "  return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muwkf_ZgEcoa"
   },
   "source": [
    "We can fine-tune our model on a limited number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "form",
    "id": "7SL2VAmVEcoa"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_2b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m training_cfg \u001b[38;5;241m=\u001b[39m TrainingConfig(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m,\n\u001b[1;32m      2\u001b[0m                               num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      3\u001b[0m                               eval_every_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m      4\u001b[0m                               batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      5\u001b[0m                               max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m params \u001b[38;5;241m=\u001b[39m train_loop(model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel_2b\u001b[49m,\n\u001b[1;32m      8\u001b[0m                     params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m'\u001b[39m]},\n\u001b[1;32m      9\u001b[0m                     train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[1;32m     10\u001b[0m                     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     11\u001b[0m                     training_cfg\u001b[38;5;241m=\u001b[39mtraining_cfg)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_2b' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
