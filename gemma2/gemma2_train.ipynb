{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5OeTiryEcoX"
   },
   "source": [
    "# Fine-tuning Gemma2 2B model on Roadrunner with JAX, Flax.\n",
    "\n",
    "We have adopted the Gemma2 notebook from Google Deepmind to use HuggingFace's libraries and and simplified the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m81VQOqEcoX"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade kagglehub -q\n",
    "!pip install ipywidgets -q\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cpu -q\n",
    "!pip install git+https://github.com/felafax/gemma.git -q\n",
    "!pip install qax -q\n",
    "!pip install jax-lorax -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_CACHE'] = '/mnt/persistent-disk/hf/'\n",
    "os.environ['HF_HOME'] = '/mnt/persistent-disk/hf/'\n",
    "!export HF_HUB_CACHE=\"/mnt/persistent-disk/hf/\"\n",
    "!export HF_HOME=\"/mnt/persistent-disk/hf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "yWaP_LPoEcoY"
   },
   "outputs": [],
   "source": [
    "# @title Python imports\n",
    "\n",
    "import enum\n",
    "import re\n",
    "import string\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# We import JAX and some related packages.\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state as ts\n",
    "from flax.traverse_util import flatten_dict\n",
    "from flax.core.meta import unbox\n",
    "\n",
    "import optax\n",
    "import functools\n",
    "from functools import partial\n",
    "\n",
    "# Model partitioning related imports\n",
    "from jax.sharding import Mesh, NamedSharding\n",
    "from jax.sharding import PartitionSpec as PS\n",
    "from jax.lax import with_sharding_constraint\n",
    "from jax.experimental import mesh_utils\n",
    "\n",
    "\n",
    "# For LoRA\n",
    "import lorax\n",
    "\n",
    "# We will use HuggingFace's dataset, tokenizer, and model classes.\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, default_data_collator\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import torch\n",
    "\n",
    "# Finally, we import Gemma.\n",
    "from gemma import params as params_lib\n",
    "from gemma import sampler as sampler_lib\n",
    "from gemma import transformer as transformer_lib\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "INPUT: Please provide your HUGGINGFACE_USERNAME:  felarof01\n",
      "INPUT: Please provide your HUGGINGFACE_TOKEN:  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\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace username and token to use when downloading.\n",
    "MODEL_NAME=\"felafax/gemma-2-2b-it-Flax\"\n",
    "HUGGINGFACE_USERNAME = input(\"INPUT: Please provide your HUGGINGFACE_USERNAME: \")\n",
    "HUGGINGFACE_TOKEN = input(\"INPUT: Please provide your HUGGINGFACE_TOKEN: \")\n",
    "\n",
    "model_name=MODEL_NAME\n",
    "hugging_face_token=HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "ckpt_path = snapshot_download(repo_id=MODEL_NAME, token=HUGGINGFACE_TOKEN)\n",
    "vocab_path = os.path.join(ckpt_path, 'tokenizer.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VsT2o6JEcoZ"
   },
   "source": [
    "## Fine tuning the Gemma model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax.traverse_util import flatten_dict\n",
    "\n",
    "def print_params(params):\n",
    "    flat_params = flatten_dict(params)    \n",
    "    for key, value in flat_params.items():\n",
    "        name = \"/\".join(str(x) for x in key)\n",
    "        print(f\"Name: {name}\")\n",
    "        # print(f\"Shape: {value.shape}\")\n",
    "        # print(f\"dtype: {value.dtype}\")\n",
    "        # print(f\"Value: {value}\")\n",
    "        if isinstance(value, flax.core.meta.Partitioned):\n",
    "            array = unbox(value)\n",
    "        else:\n",
    "            array = value\n",
    "        print(jax.debug.visualize_array_sharding(array))\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: prepare the dataset\n",
    "\n",
    "For this project, we're utilizing the refined **Alpaca dataset**, curated by yahma. This dataset is a carefully filtered selection of 52,000 entries from the original Alpaca collection. Feel free to substitute this section with your own data preparation code if you prefer.\n",
    "\n",
    "It's crucial to include the EOS_TOKEN (End of Sequence Token) in your tokenized output. Failing to do so may result in endless generation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(*, tokenizer, batch_size=1, max_length=32, max_examples=32):\n",
    "    # Define Alpaca prompt template\n",
    "    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "    \n",
    "    ### Instruction: {}\n",
    "    \n",
    "    ### Input: {}\n",
    "    \n",
    "    ### Response: {}\"\"\"\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    \n",
    "    # Define formatting function.\n",
    "    def _format_prompts(examples):\n",
    "        instructions = examples[\"instruction\"]\n",
    "        inputs = examples[\"input\"]\n",
    "        outputs = examples[\"output\"]\n",
    "        texts = []\n",
    "        for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "            text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    def _tokenize(examples):\n",
    "        tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length+1)\n",
    "        tokenized['input_ids'] = [input_id[:-1] for input_id in tokenized['input_ids']]\n",
    "        tokenized['target_mask'] = [input_id[:-1] for input_id in tokenized['attention_mask']]\n",
    "        return {\n",
    "            'input_tokens': tokenized['input_ids'],\n",
    "            'target_mask': tokenized['target_mask']\n",
    "        }\n",
    "\n",
    "    def _custom_collate_fn(batch):\n",
    "        \"\"\"Applies default_collate_fn from transformers and converts to JAX NumPy arrays.\"\"\"\n",
    "        batch = default_data_collator(batch)\n",
    "        jax_batch = {}\n",
    "        for key, value in batch.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                jax_batch[key] = jnp.array(value.numpy())\n",
    "            else:\n",
    "                jax_batch[key] = value\n",
    "        \n",
    "        return jax_batch\n",
    "\n",
    "    # Load and preprocess the dataset.\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "    if max_examples:\n",
    "        dataset = dataset.select(range(max_examples))\n",
    "    dataset = dataset.map(_format_prompts, batched=True)\n",
    "\n",
    "    # Create train and test dataset.\n",
    "    ds = dataset.train_test_split(test_size=0.15)\n",
    "    ds['train'] = ds['train'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "    ds['test'] = ds['test'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['train'],\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=_custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['test'],\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=_custom_collate_fn\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset loader by extracting a batch.\n",
    "# train_dataloader, _ = get_dataset(tokenizer=tokenizer)\n",
    "# for i, batch in enumerate(train_dataloader):\n",
    "#     if i>5:\n",
    "#         break\n",
    "#     input_ids, attention_mask = (\n",
    "#         batch[\"input_tokens\"],\n",
    "#         batch[\"target_mask\"],\n",
    "        \n",
    "#     )\n",
    "#     print(input_ids.shape)\n",
    "#     print()\n",
    "#     print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "form",
    "id": "iEcV0XEEEcoZ"
   },
   "outputs": [],
   "source": [
    "def forward_and_loss_fn(params,\n",
    "                        *,\n",
    "                        state,\n",
    "                        input_tokens: jax.Array,            # Shape [B, L]\n",
    "                        input_mask: jax.Array,              # Shape [B, L]\n",
    "                        positions: jax.Array,               # Shape [B, L]\n",
    "                        attention_mask: jax.Array,          # [B, L, L]\n",
    "                        ) -> jax.Array:\n",
    "  \"\"\"Forward pass and loss function.\n",
    "\n",
    "  Args:\n",
    "    params: model's input parameters.\n",
    "    model: gemma transformer model to call.\n",
    "    input_tokens: input tokens sequence, shape [B, L].\n",
    "    input_mask: tokens to ignore when computing the loss, shape [B, L].\n",
    "    positions: relative position of each token, shape [B, L].\n",
    "    attention_mask: input attention mask, shape [B, L].\n",
    "\n",
    "  Returns:\n",
    "    Softmax cross-entropy loss for the next-token prediction task.\n",
    "  \"\"\"\n",
    "\n",
    "  # Forward pass on the input data.\n",
    "  # No attention cache is needed here.\n",
    "  logits, _ = state.apply_fn(\n",
    "        params,\n",
    "        input_tokens,\n",
    "        positions,\n",
    "        None,              # Attention cache is None.\n",
    "        attention_mask,\n",
    "    )\n",
    "\n",
    "  # Exclude the last step as it does not appear in the targets.\n",
    "  logits = logits[:, :-1]\n",
    "\n",
    "  # Similarly, the first token cannot be predicteds.\n",
    "  target_tokens = input_tokens[:, 1:]\n",
    "  target_mask = input_mask[:, 1:]\n",
    "\n",
    "  # Convert the target labels into one-hot encoded vectors.\n",
    "  one_hot = jax.nn.one_hot(target_tokens, logits.shape[-1])\n",
    "\n",
    "  # Don't update on unwanted tokens.\n",
    "  one_hot = one_hot * target_mask.astype(one_hot.dtype)[..., None]\n",
    "\n",
    "  # Normalisation factor.\n",
    "  norm_factor = 1 / (jnp.sum(target_mask) + 1e-8)\n",
    "\n",
    "  # Return the nll loss.\n",
    "  return -jnp.sum(jax.nn.log_softmax(logits) * one_hot) * norm_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y83DimpjEcoZ"
   },
   "source": [
    "The Gemma transformer requires an attention mask and position vector alongside each input. We can conveniently generate these using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "form",
    "id": "cbWfdHf0EcoZ"
   },
   "outputs": [],
   "source": [
    "def get_attention_mask_and_positions(example: jax.Array,\n",
    "                                     pad_id : int,\n",
    "                                     )-> tuple[jax.Array, jax.Array]:\n",
    "  \"\"\"Builds the position and attention mask vectors from the given tokens.\"\"\"\n",
    "  pad_mask = example != pad_id\n",
    "  current_token_position = transformer_lib.build_positions_from_mask(pad_mask)\n",
    "  attention_mask = transformer_lib.make_causal_attn_mask(pad_mask)\n",
    "  return current_token_position, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbxYMMWLEcoZ"
   },
   "source": [
    "We can now build the train_step function which performs the backward pass and updates the model's parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "cellView": "form",
    "id": "cPSfp7ZUEcoZ"
   },
   "outputs": [],
   "source": [
    "def train_step(state,\n",
    "               params,\n",
    "               # optimizer: optax.GradientTransformation,\n",
    "               # opt_state: optax.OptState,\n",
    "               pad_id: int,\n",
    "               batch):\n",
    "  \"\"\"Train step.\n",
    "\n",
    "  Args:\n",
    "    model: gemma transformer model.\n",
    "    params: model's input parameters.\n",
    "    optimizer: optax optimizer to use.\n",
    "    opt_state: input optimizer's state.\n",
    "    pad_id: id of the pad token.\n",
    "    batch: input batch.\n",
    "\n",
    "  Returns:\n",
    "    Training loss, updated parameters, updated optimizer state.\n",
    "  \"\"\"\n",
    "  # Build the position and attention mask vectors.\n",
    "  positions, attention_mask = get_attention_mask_and_positions(batch['input_tokens'], pad_id)\n",
    "\n",
    "  # Forward and backward passes\n",
    "  train_loss, grads = jax.value_and_grad(forward_and_loss_fn)(params,\n",
    "                                                             state=state,\n",
    "                                                             input_tokens=batch['input_tokens'],\n",
    "                                                             input_mask=batch['target_mask'],\n",
    "                                                             positions=positions,\n",
    "                                                             attention_mask=attention_mask)\n",
    "  # Update the parameters\n",
    "  # updates, opt_state = optimizer.update(grads, opt_state)\n",
    "  # params = optax.apply_updates(params, updates)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "\n",
    "  return train_loss, params, opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2QXp116EcoZ"
   },
   "source": [
    "Similarly, we build a `validation_step` function without backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6g6LFWJbEcoa"
   },
   "source": [
    "And now the training loop itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the device mesh\n",
    "devices = jax.devices()\n",
    "device_mesh = mesh_utils.create_device_mesh((1, 4, 1))\n",
    "mesh = Mesh(devices=device_mesh, axis_names=('data', 'model', 'replica'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "  learning_rate: float\n",
    "  num_epochs: int\n",
    "  eval_every_n: int\n",
    "  batch_size: int\n",
    "  max_steps: int | None = None\n",
    "\n",
    "training_cfg = TrainingConfig(learning_rate=1e-4,\n",
    "                              num_epochs=1,\n",
    "                              eval_every_n=20,\n",
    "                              batch_size=1,\n",
    "                              max_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters.\n",
    "params = {\"params\": params_lib.load_and_format_params(os.path.join(ckpt_path, 'gemma2-2b-it'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model config.\n",
    "config = transformer_lib.TransformerConfig.gemma2_2b(cache_size=30)\n",
    "model = transformer_lib.Transformer(config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    token=HUGGINGFACE_TOKEN\n",
    ")\n",
    "optimizer = optax.sgd(training_cfg.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainstate_from_params(params, optimizer):\n",
    "    state = ts.TrainState.create(\n",
    "        apply_fn=None,\n",
    "        params=params['params'],\n",
    "        tx=optimizer)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_params_pytree(params, mesh):\n",
    "    def shard_param(param):\n",
    "        if len(param.shape) == 0:\n",
    "            return NamedSharding(mesh, PS())\n",
    "        elif len(param.shape) == 1:\n",
    "            return NamedSharding(mesh, PS('model'))\n",
    "        elif len(param.shape) == 2:\n",
    "            return NamedSharding(mesh, PS('data', 'model'))\n",
    "        elif len(param.shape) == 3:\n",
    "            return NamedSharding(mesh, PS('data', 'model', 'replica'))\n",
    "        else:\n",
    "            # For higher-dimensional tensors, you might need a more complex strategy\n",
    "            return NamedSharding(mesh, PS(None))  # Replicate by default\n",
    "\n",
    "    return jax.tree_util.tree_map(shard_param, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state = create_trainstate_from_params(params, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state_shapes = jax.eval_shape(\n",
    "    functools.partial(\n",
    "        create_trainstate_from_params,\n",
    "        params=params,\n",
    "        optimizer=optimizer,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state_partition = shard_params_pytree(\n",
    "    train_state_shapes, mesh\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharded_create_trainstate_from_params = jax.jit(\n",
    "    create_trainstate_from_params,\n",
    "    in_shardings=(train_state_partition.params, ),\n",
    "    # out_shardings=train_state_partition,\n",
    "    donate_argnums=(0, ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharded_train_step = jax.jit(\n",
    "    train_step,\n",
    "    in_shardings=(train_state_partition, train_state_partition.params, NamedSharding(mesh, PS()), NamedSharding(mesh, PS())),\n",
    "    # out_shardings=(train_state_partition),\n",
    "    donate_argnums=(0, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mesh:\n\u001b[1;32m      2\u001b[0m     sample_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_dataloader))\n\u001b[0;32m----> 3\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43msharded_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[55], line 24\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(state, params, pad_id, batch)\u001b[0m\n\u001b[1;32m     21\u001b[0m positions, attention_mask \u001b[38;5;241m=\u001b[39m get_attention_mask_and_positions(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m], pad_id)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Forward and backward passes\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m train_loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_and_loss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43minput_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Update the parameters\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# updates, opt_state = optimizer.update(grads, opt_state)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# params = optax.apply_updates(params, updates)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mapply_gradients(grads\u001b[38;5;241m=\u001b[39mgrads)\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[30], line 25\u001b[0m, in \u001b[0;36mforward_and_loss_fn\u001b[0;34m(params, state, input_tokens, input_mask, positions, attention_mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass and loss function.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m  Softmax cross-entropy loss for the next-token prediction task.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Forward pass on the input data.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# No attention cache is needed here.\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Attention cache is None.\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Exclude the last step as it does not appear in the targets.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "with mesh:\n",
    "    sample_batch = next(iter(train_dataloader))\n",
    "    state = sharded_train_step(train_state, train_state.params, tokenizer.pad_token_id, sample_batch)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
